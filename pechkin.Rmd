---
title: "Pechkin"
output: html_notebook
---

```{r}
library(tidyverse)
library(priceR)
library(lubridate)

raw_data <- read_csv("data/2023-07-15.csv")
with_prices_data <- raw_data %>% filter(!is.na(price))

normalized_data <- with_prices_data %>% mutate(
  `Новостройка` = case_when(
    `Новостройка` == "Да" ~ TRUE,
    `Новостройка` == "Нет" ~ FALSE
  ),
  
  `Лифт` = ifelse(`Лифт` == "Есть", TRUE, FALSE),
  price = str_replace_all(price, ",", ""),
  amount = as.numeric(str_extract(price, "\\d+")),
  currency = trimws(str_extract(price, "\\D+")),
  currency = case_when(
    currency == "$" ~ "USD",
    currency == "֏" ~ "AMD",
    currency == "€" ~ "EUR",
    TRUE ~ currency
  ),
  price_usd = convert_currencies(price_start = amount, from=currency, to="USD"),
  area = as.numeric(str_extract(`Общая площадь`, "\\d+")),
  height = as.numeric(str_extract(`Высота потолков`, "\\d+(.\\d+)?")),
  rooms=parse_number(`Количество комнат`),
  bathrooms=parse_number(`Количество санузлов`),
  updated_at=parse_date_time(updated_at, "dmY HM"),
  posted_at=as_date(posted_at),
  updated_at=as_date(updated_at),
  date=pmax(posted_at, updated_at, na.rm = TRUE)
) %>% select(-price, -amount, -currency, -`Общая площадь`, -`Высота потолков`, -`Количество комнат`, -`Количество санузлов`, -posted_at, -updated_at)

normalized_data %>% summarise(min_value=min(price_usd), max_value=max(price_usd), mean_value=mean(price_usd))
```

```{r}
normalized_data %>%
  filter(price_usd > 50000) %>% summarise(min_value=min(price_usd))
```

```{r}
breaks <- c(50000, 100000, 150000, 200000, 300000, 500000, 1000000, 4000000)

normalized_data <- normalized_data %>%
  filter(price_usd > 50000) %>%
  mutate(priceRange = cut(price_usd, breaks=breaks, lables=FALSE, include.lowest=TRUE, dig.lab=7))

normalized_data %>% count(priceRange)
```
```{r}
normalized_data
```

```{r}
hex_plot <- normalized_data %>%
  ggplot(aes(longitude, latitude, z=price_usd)) +
  stat_summary_hex(alpha=0.8, bins=50) +
  scale_fill_viridis_c() + 
  labs(
    fill = "mean",
    title = "Price"
  )
hex_plot
```

```{r}
library(osmdata)
library(ggmap)

yerevan_bb <- getbb("Kentron")

yerevan_map <- get_stamenmap(yerevan_bb, zoom = 15)

ggmap(yerevan_map) +
  stat_summary_hex(
    data = normalized_data,
    aes(longitude, latitude, z = price_usd),
    alpha = 0.5,
    bins = 50
  ) +
  scale_fill_viridis_c() +
  labs(fill = "mean",
       title = "Price")
```

```{r}
library(dplyr)
library(ggplot2)
set.seed(23072023)
num_clusters <- 10

clustered_df <- normalized_data %>%
  select(latitude, longitude) %>%
  kmeans(nstart = 25, centers = num_clusters)

clustered <- normalized_data
clustered$cluster <- as.factor(clustered_df$cluster)

ggplot(clustered, aes(x = longitude, y = latitude, color = cluster)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Set3") +
  labs(title = "Location Clustering")
```
```{r}
not_in_yerevan <- clustered %>% filter(cluster %in% c(6,9))
not_in_yerevan
```
```{r}
apts_in_yerevan <- normalized_data %>% filter(!(id %in% not_in_yerevan$id))
apts_in_yerevan %>%
  ggplot(aes(longitude, latitude, z=price_usd)) +
  stat_summary_hex(alpha=0.8, bins=50) +
  scale_fill_viridis_c() + 
  labs(
    fill = "mean",
    title = "Price"
  )
```
```{r}
library(patchwork)

plot <- function(df, var, title) {
  df %>%
    ggplot(aes(longitude, latitude, z = {
      {
        var
      }
    })) +
    stat_summary_hex(alpha = 0.8, bins = 50) +
    scale_fill_viridis_c() +
    labs(fill = "mean",
         title = title)
}

(plot(apts_in_yerevan, price_usd, "Price") +
    plot(apts_in_yerevan, area, "Area")) /
  (
    plot(apts_in_yerevan, rooms, "Rooms") +
      plot(apts_in_yerevan, bathrooms, "Bathrooms")
  )
```
```{r}
library(tidytext)

yerevan_tidy <- apts_in_yerevan %>%
  mutate(priceRange = as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", priceRange) ) ) %>%
  unnest_tokens(word, summary) %>%
  anti_join(get_stopwords("ru"))

yerevan_tidy %>%
  count(word, sort = TRUE)
```

```{r}
top_words <-
  yerevan_tidy %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% as.character(1:1000)) %>%
  slice_max(n, n = 100) %>%
  pull(word)

word_freqs <-
  yerevan_tidy %>%
  count(word, priceRange) %>%
  complete(word, priceRange, fill = list(n = 0)) %>%
  group_by(priceRange) %>%
  mutate(price_total = sum(n),
         proportion = n / price_total) %>%
  ungroup() %>%
  filter(word %in% top_words)

word_freqs
```
```{r}
word_mods <-
  word_freqs %>%
  nest(data = c(priceRange, n, price_total, proportion)) %>%
  mutate(model = map(data, ~ glm(
    cbind(n, price_total) ~ priceRange, ., family = "binomial"
  )),
  model = map(model, tidy)) %>%
  unnest(model) %>%
  filter(term == "priceRange")  %>%
  mutate(p.value = p.adjust(p.value)) %>%
  arrange(-estimate)

word_mods
```
```{r}
library(ggrepel)

word_mods %>%
  ggplot(aes(estimate, p.value)) +
  geom_vline(
    xintercept = 0,
    lty = 2,
    alpha = 0.7,
    color = "gray50"
  ) +
  geom_point(color = "midnightblue",
             alpha = 0.8,
             size = 2.5) +
  scale_y_log10() +
  geom_text_repel(aes(label = word))
```
```{r}
higher_words <-
  word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(estimate, n = 12) %>%
  pull(word)

lower_words <-
  word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(-estimate, n = 12) %>%
  pull(word)
```

```{r}
word_freqs %>%
  filter(word %in% lower_words) %>%
  ggplot(aes(priceRange, proportion, color = word)) + 
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free_y") +
  scale_x_continuous(labels = scales::dollar)  +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA))  +
  labs(x = NULL, y = "proportion of total words used for homes at that price") +
  theme_light()
```
```{r}
library(tidymodels)
library(stringr)
set.seed(20230729)

yerevan_split <- apts_in_yerevan %>%
  select(-url, -price_usd, -location) %>%
  mutate(summary=str_to_lower(summary)) %>%
  initial_split(strata=priceRange)

yerevan_train <- training(yerevan_split)
yerevan_test <- testing(yerevan_split)

set.seed(29072023)
yerevan_folds <- vfold_cv(yerevan_train, v=5, strata = priceRange)
```

```{r}
higher_pat <- glue::glue_collapse(higher_words, sep = "|")
lower_pat <- glue::glue_collapse(lower_words, sep = "|")

yerevan_rec <- 
  recipe(priceRange ~ ., data=yerevan_train) %>%
  update_role(id, new_role = "uid") %>%
  step_regex(summary, pattern = higher_pat, result = "high_price_words") %>%
  step_regex(summary, pattern = lower_pat, result = "low_price_words")  %>%
  step_rm(summary) %>%
  step_mutate(`Новостройка`=as.numeric(`Новостройка`)) %>%
  step_mutate(`Лифт`=as.numeric(`Лифт`)) %>%
  step_date(date, features = c("month", "year"), keep_original_cols = FALSE) %>%
  step_novel(`Тип здания`, `Балкон`, `Мебель`, `Ремонт`) %>%
  step_unknown(`Тип здания`, `Балкон`, `Мебель`, `Ремонт`) %>%
  step_other(`Тип здания`, `Балкон`, `Мебель`, `Ремонт`, threshold = 0.02) %>%
  step_dummy_extract(`Парковка`, `Бытовая техника`, `Виды из окон`, sep=", ") %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())
yerevan_rec
```

```{r}
xgb_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    mtry = tune(),
    sample_size = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_word_wf <- workflow(yerevan_rec, xgb_spec)

set.seed(20230729)

xgb_grid <-
  grid_max_entropy(
    tree_depth(c(5L, 10L)),
    min_n(c(10L, 40L)),
    mtry(c(5L, 10L)),
    sample_prop(c(0.5, 1.0)),
    learn_rate(c(-2, -1)),
    size = 20
  )

xgb_grid
```
```{r}
library(finetune)
doParallel::registerDoParallel()

set.seed(29072023)
xgb_word_rs <-
  tune_race_anova(
    xgb_word_wf,
    yerevan_folds,
    grid = xgb_grid,
    metrics = metric_set(mn_log_loss),
    control = control_race(verbose_elim = TRUE)
  )

xgb_word_rs
```
```{r}
plot_race(xgb_word_rs)
```
```{r}
show_best(xgb_word_rs)
```

```{r}
xgb_last <-
  xgb_word_wf %>%
  finalize_workflow(select_best(xgb_word_rs, "mn_log_loss")) %>%
  last_fit(yerevan_split)

xgb_last
```

```{r}
collect_predictions(xgb_last) %>%
  mn_log_loss(priceRange,
              `.pred_[50000,100000]`:`.pred_(1000000,4000000]`)
```
```{r}
collect_predictions(xgb_last) %>%
  conf_mat(priceRange, .pred_class) %>%
  autoplot()
```

```{r}
collect_predictions(xgb_last) %>%
  roc_curve(priceRange,
            `.pred_[50000,100000]`:`.pred_(1000000,4000000]`) %>%
  ggplot(aes(1 - specificity, sensitivity, color = .level)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(alpha = 0.8, size = 1.2) +
  coord_equal() +
  labs(color = NULL)
```

```{r}
library(vip)
extract_workflow(xgb_last) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point", num_features = 20)
```
